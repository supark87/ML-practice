{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "yoga=tf.constant('yoga world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(yoga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.constant(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'yoga world'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(yoga)\n",
    "#b = indication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##operations\n",
    "x = tf.constant(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= tf.constant(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations with Constants\n",
      "Addition:  5\n",
      "Subtraction: -1\n",
      "Multiplication: 6\n",
      "Division: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('Operations with Constants')\n",
    "    print('Addition: ',sess.run(x+y))\n",
    "    print('Subtraction:', sess.run(x-y))\n",
    "    print('Multiplication:',sess.run(x*y))\n",
    "    print('Division:',sess.run(x/y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_6:0' shape=<unknown> dtype=float64>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "add = tf.add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.framework.ops.Tensor"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = tf.subtract(x,y)\n",
    "mul = tf.multiply(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {x:20, y:30}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations with Placeholders\n",
      "addition 50.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print('Operations with Placeholders')\n",
    "    print('addition',sess.run(add,feed_dict=d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[5.0,5.0]])\n",
    "b = np.array([[2.0],[2.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1 = tf.constant(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat2 = tf.constant(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_multi = tf.matmul(mat1,mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    result = sess.run(matrix_multi)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.contrib.learn.python.learn.datasets.base.Datasets"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images.shape\n",
    "#55000 images, 784 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.images[1].shape\n",
    "##need to be reshaped as (28,28) to see image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x131b5d2e8>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131a59e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[1].reshape(28,28),cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x131c4ed68>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAD4AAAD8CAYAAAAv4Rf7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n39xCIwoPjSg8NIIV/n8WJn24ukja6gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x131b87128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[1].reshape(784,1),cmap='gist_gray',aspect=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break down batches\n",
    "x = tf.placeholder(tf.float32,shape=[None,784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weights\n",
    "#784 pixels and 10 possible labels\n",
    "W = tf.Variable(tf.zeros([784,10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup bias\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN function formula\n",
    "y=tf.matmul(x,W)+ b\n",
    "#prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passing true labels\n",
    "y_true =tf.placeholder(tf.float32,shape=[None,10])\n",
    "#why None and 10 \n",
    "# 10 : possible label. so if index is 1, that number is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y))\n",
    "# this minimizes error between y_true and y\n",
    "#define the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5)\n",
    "# less learning rate : more accurate but slower\n",
    "#reduce the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = optimizer.minimize(cross_entropy)\n",
    "#grab the error and minimize it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.glorot_normal_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument <tensorflow.python.ops.init_ops.VarianceScaling object at 0x13c1e26a0> has invalid type <class 'tensorflow.python.ops.init_ops.VarianceScaling'>, must be a string or Tensor. (Can not convert a VarianceScaling into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    270\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 271\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    272\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3035\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   3123\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\" % (type(obj).__name__,\n\u001b[0;32m-> 3124\u001b[0;31m                                                            types_str))\n\u001b[0m\u001b[1;32m   3125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a VarianceScaling into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-ffb1efcfc574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1105\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    412\u001b[0m     \"\"\"\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    273\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[1;32m    274\u001b[0m                         \u001b[0;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                         % (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[1;32m    276\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument <tensorflow.python.ops.init_ops.VarianceScaling object at 0x13c1e26a0> has invalid type <class 'tensorflow.python.ops.init_ops.VarianceScaling'>, must be a string or Tensor. (Can not convert a VarianceScaling into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)   \n",
    "    for step in range(1000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(1000)\n",
    "        sess.run(train,feed_dict={x:batch_x,y_true:batch_y})\n",
    "        #x and y defied as place_holder until passing it as dictionary\n",
    "    matches = tf.equal(tf.argmax(y,1),tf.argmax(y_true,1))\n",
    "    \n",
    "    acc = tf.reduce_mean(tf.cast(matches,tf.float32))\n",
    "    print(sess.run(acc,feed_dict ={x: mnist.test.images,y_true:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tensor Flow estimators\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "iris = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
